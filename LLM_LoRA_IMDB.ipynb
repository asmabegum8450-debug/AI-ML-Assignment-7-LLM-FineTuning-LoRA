{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Fine-Tuning DistilBERT with LoRA for IMDB Sentiment Classification"]}, {"cell_type": "code", "metadata": {}, "source": ["# Install dependencies (uncomment if needed)\n", "# !pip install transformers datasets peft accelerate evaluate torch"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["import os, random, numpy as np, torch\n", "from datasets import load_dataset\n", "import evaluate\n", "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n", "from peft import LoraConfig, TaskType, get_peft_model\n", "\n", "SEED = 42\n", "random.seed(SEED)\n", "np.random.seed(SEED)\n", "torch.manual_seed(SEED)\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print('Using device:', device)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["dataset = load_dataset('imdb')\n", "SUB_TRAIN, SUB_TEST = 8000, 2000\n", "dataset['train'] = dataset['train'].shuffle(seed=SEED).select(range(SUB_TRAIN))\n", "dataset['test'] = dataset['test'].shuffle(seed=SEED).select(range(SUB_TEST))\n", "\n", "split = dataset['train'].train_test_split(test_size=0.1, seed=SEED)\n", "train, val, test = split['train'], split['test'], dataset['test']"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["model_name = 'distilbert-base-uncased'\n", "tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n", "\n", "def tokenize(batch):\n", "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n", "\n", "train_tok = train.map(tokenize, batched=True, remove_columns=['text'])\n", "val_tok   = val.map(tokenize, batched=True, remove_columns=['text'])\n", "test_tok  = test.map(tokenize, batched=True, remove_columns=['text'])\n", "\n", "for ds in [train_tok, val_tok, test_tok]:\n", "    ds.set_format(type='torch', columns=['input_ids','attention_mask','label'])"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n", "\n", "lora = LoraConfig(task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1,\n", "                  target_modules=['q_lin','v_lin'])\n", "model = get_peft_model(model, lora).to(device)\n", "\n", "def print_trainable(model):\n", "    t, a = 0, 0\n", "    for p in model.parameters():\n", "        a += p.numel()\n", "        if p.requires_grad: t += p.numel()\n", "    print(f'Trainable: {t} / {a} ({100*t/a:.2f}%)')\n", "\n", "print_trainable(model)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["acc = evaluate.load('accuracy')\n", "f1 = evaluate.load('f1')\n", "def metrics(eval_pred):\n", "    logits, labels = eval_pred\n", "    preds = np.argmax(logits, axis=-1)\n", "    return {\n", "        'accuracy': acc.compute(predictions=preds, references=labels)['accuracy'],\n", "        'f1': f1.compute(predictions=preds, references=labels)['f1'],\n", "    }"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["args = TrainingArguments(\n", "    output_dir='lora-distilbert-imdb',\n", "    per_device_train_batch_size=16,\n", "    per_device_eval_batch_size=32,\n", "    num_train_epochs=2,\n", "    learning_rate=5e-4,\n", "    evaluation_strategy='epoch',\n", "    save_strategy='epoch',\n", "    load_best_model_at_end=True,\n", "    metric_for_best_model='accuracy'\n", ")\n", "\n", "trainer = Trainer(model=model, args=args, train_dataset=train_tok,\n", "                  eval_dataset=val_tok, tokenizer=tokenizer,\n", "                  data_collator=data_collator, compute_metrics=metrics)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["trainer.train()"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["print('Validation:', trainer.evaluate(eval_dataset=val_tok))\n", "print('Test:', trainer.evaluate(eval_dataset=test_tok))"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {}, "source": ["def classify(text):\n", "    model.eval()\n", "    inputs = tokenizer(text, return_tensors='pt', truncation=True,\n", "                       padding=True, max_length=256).to(device)\n", "    with torch.no_grad():\n", "        logits = model(**inputs).logits\n", "    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n", "    label = 'pos' if np.argmax(probs)==1 else 'neg'\n", "    return {'text': text, 'label': label, 'confidence': float(max(probs))}\n", "\n", "print(classify('This movie was fantastic!'))\n", "print(classify('The movie had moments but was boring overall.'))"], "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 5}